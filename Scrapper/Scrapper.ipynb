import requests
import re
import os
import time
import pandas as pd
from bs4 import BeautifulSoup
import json

def split_line(line, limit=20):
    words = line.split()
    return "\n".join(" ".join(words[i:i+limit]) for i in range(0, len(words), limit))

def safe_filename(text):
    return re.sub(r'[^\w\s-]', '', text).strip().replace(' ', '_')

def save_progress(year, papers_data, csv_path):
    papers_df = pd.DataFrame(papers_data)
    papers_df.to_csv(csv_path, index=False)
    with open("progress.json", "w") as f:
        json.dump({"last_year": year}, f)
    print(f"Progress saved. Last completed year: {year}")

def load_progress():
    try:
        with open("progress.json", "r") as f:
            return json.load(f).get("last_year", 1987)
    except FileNotFoundError:
        return 1987

def scrape_paper(url, out_dir, papers_data):
    headers = {"User-Agent": "Mozilla/5.0"}
    print(f"Scraping: {url}")
    res = requests.get(url, headers=headers)
    if res.status_code != 200:
        print(f"Failed {url}: {res.status_code}")
        return
    soup = BeautifulSoup(res.text, "html.parser")
    title_elem = soup.find("h4")
    title = title_elem.get_text(strip=True) if title_elem else "output"
    safe_title = safe_filename(title)

    os.makedirs(out_dir, exist_ok=True)
    container = soup.find("div", class_="container-fluid")
    col = container.find("div", class_="col") if container else soup
    elems = col.find_all(["h4", "p"])
    texts = [e.get_text(" ", strip=True) for e in elems if e.get_text(" ", strip=True)]
    processed = [split_line(t) if len(t.split()) > 20 else t for t in texts]
    final_text = "\n".join(processed)
    
    file_links = {}
    for a in col.find_all("a", href=True):
        txt = a.get_text(strip=True)
        if txt in {"Bibtex", "Paper", "Supplemental"}:
            href = a["href"]
            if href.startswith("/"):
                href = "https://papers.nips.cc" + href
            file_links[txt] = href
    
    paper_data = {
        "Title": title,
        "Abstract": final_text,
        "Bibtex": file_links.get("Bibtex", ""),
        "Paper Link": file_links.get("Paper", ""),
        "Supplemental": file_links.get("Supplemental", ""),
    }
    papers_data.append(paper_data)
    
    # Download PDFs and ZIPs
    for a in col.find_all("a", href=True):
        href = a["href"]
        if href.lower().endswith((".pdf", ".zip")):
            file_link = href if href.startswith("http") else "https://papers.nips.cc" + href
            file_extension = file_link.split('.')[-1]
            file_path = os.path.join(out_dir, f"{safe_title}.{file_extension}")
            print(f"Downloading file: {file_link}")
            try:
                file_res = requests.get(file_link, headers=headers, stream=True)
                if file_res.status_code == 200:
                    with open(file_path, "wb") as f:
                        for chunk in file_res.iter_content(1024):
                            if chunk:
                                f.write(chunk)
                    print(f"Saved file: {file_path}")
                else:
                    print(f"Failed to download: {file_link}")
            except requests.exceptions.RequestException as e:
                print(f"Error downloading {file_link}: {e}")

def get_links(year):
    base_url = f"https://papers.nips.cc/paper_files/paper/{year}"
    headers = {"User-Agent": "Mozilla/5.0"}
    print(f"Fetching links for {year} from {base_url}")
    try:
        res = requests.get(base_url, headers=headers)
        if res.status_code != 200:
            print(f"Failed index for {year}: {res.status_code}")
            return []
        soup = BeautifulSoup(res.text, "html.parser")
        return ["https://papers.nips.cc" + a["href"] for a in soup.find_all("a", href=True) if "hash" in a["href"]]
    except requests.exceptions.RequestException as e:
        print(f"Error fetching links for {year}: {e}")
        return []

def scrape_papers_to_csv(start_year, end_year):
    csv_path = "neurips_papers.csv"
    last_completed_year = load_progress()
    papers_data = []

    for year in range(last_completed_year, end_year + 1):
        print(f"\n=== Year {year} ===")
        out_dir = f"NeurIPS_{year}"
        links = get_links(year)
        if not links:
            print(f"No links for {year}")
            continue
        for i, link in enumerate(links, 1):
            print(f"\nPaper {i}/{len(links)} for {year}:")
            try:
                scrape_paper(link, out_dir, papers_data)
            except Exception as e:
                print(f"Error on {link}: {e}")
            time.sleep(1)
        save_progress(year, papers_data, csv_path)

scrape_papers_to_csv(1987, 2010)